# ==========================================================
# ATHLETIC DIET RECOMMENDATION SYSTEM - MODEL TRAINING SCRIPT
# ==========================================================
# Hybrid Architecture:
# - Random Forest (Supervised Learning)
# - K-Means (Unsupervised Learning)
# - Meal Time Classification (Breakfast/Lunch/Dinner/Snack)
# - Integrated for Personalized Food Recommendations
# ==========================================================

import pandas as pd
import numpy as np
import joblib
import warnings
import sys
import ast

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.cluster import KMeans
from sklearn.metrics import (
    accuracy_score, 
    classification_report, 
    silhouette_score,
    precision_recall_fscore_support,
    confusion_matrix
)

from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

# Streamlined output
warnings.filterwarnings('ignore', category=UserWarning)
sys.stdout.reconfigure(line_buffering=True)


# ==========================================================
# PART 0 ‚Äì LOAD ATHLETE DATA
# ==========================================================
def load_and_explore_data(filepath):
    """Load the athlete dataset for Random Forest training"""
    try:
        df = pd.read_csv(filepath)
        print(f"‚úÖ '{filepath}' loaded successfully.")
        print(f"Shape: {df.shape}")
        print("Columns:", df.columns.tolist())
        return df
    except FileNotFoundError:
        print(f"‚ùå Error: '{filepath}' not found.")
        return None


# ==========================================================
# PART 0A ‚Äì MEAL TIME IDENTIFICATION SYSTEM
# ==========================================================
def identify_meal_time(food_name, calories, proteins, carbs, fats):
    """
    Identify when a food is typically eaten (Breakfast, Lunch, Dinner, Snack).
    Based on research: USDA NHANES, International Breakfast Research Initiative, 
    and Dietary Guidelines for meal classification.
    
    Returns: meal_time (string)
    """
    food_name_lower = str(food_name).lower()
    
    # Define keyword dictionaries for meal time classification
    # Based on: International Breakfast Research Initiative (2018) - PMC Studies
    BREAKFAST_KEYWORDS = [
        'oatmeal', 'cereal', 'pancake', 'waffle', 'toast', 'bagel', 'muffin',
        'breakfast', 'scrambled', 'omelet', 'omelette', 'egg', 'bacon',
        'sausage', 'granola', 'yogurt', 'smoothie', 'coffee', 'juice',
        'croissant', 'porridge', 'french toast', 'crepe', 'hash brown'
    ]
    
    # Based on: USDA meal classification standards
    LUNCH_KEYWORDS = [
        'sandwich', 'wrap', 'salad', 'soup', 'burger', 'pizza', 'pasta',
        'noodle', 'rice bowl', 'burrito', 'taco', 'sub', 'panini',
        'quesadilla', 'pita', 'flatbread', 'lunch', 'deli'
    ]
    
    # Based on: Dietary Guidelines typical dinner foods
    DINNER_KEYWORDS = [
        'steak', 'chicken breast', 'salmon', 'fish fillet', 'roast', 'grilled',
        'baked chicken', 'baked fish', 'stir fry', 'curry', 'casserole', 
        'roasted', 'braised', 'pot roast', 'dinner', 'ribeye', 'pork chop',
        'lamb chop', 'filet', 'tenderloin', 'prime rib'
    ]
    
    SNACK_KEYWORDS = [
        'bar', 'chip', 'cracker', 'nut', 'seed', 'trail mix', 'popcorn',
        'pretzel', 'cookie', 'brownie', 'cake', 'candy', 'chocolate',
        'fruit', 'berry', 'apple', 'banana', 'orange', 'grape', 'snack',
        'protein bar', 'energy bar', 'granola bar'
    ]
    
    # Check for meal time keywords in order of specificity
    # 1. Check Breakfast
    for keyword in BREAKFAST_KEYWORDS:
        if keyword in food_name_lower:
            return 'Breakfast'
    
    # 2. Check Snack (before lunch/dinner to catch snack items)
    for keyword in SNACK_KEYWORDS:
        if keyword in food_name_lower:
            return 'Snack'
    
    # 3. Check Lunch
    for keyword in LUNCH_KEYWORDS:
        if keyword in food_name_lower:
            return 'Lunch'
    
    # 4. Check Dinner
    for keyword in DINNER_KEYWORDS:
        if keyword in food_name_lower:
            return 'Dinner'
    
    # If no keyword match, use nutritional profile
    # Based on: 2020 Dietary Guidelines - energy distribution across meals
    # Breakfast: 18%, Lunch: 27%, Dinner: 32%, Snacks: 23%
    
    # High protein (>25g) + substantial calories (>300) = Dinner
    if proteins > 25 and calories > 300:
        return 'Dinner'
    
    # High protein (>15g) + moderate calories = Lunch
    elif proteins > 15 and 200 < calories <= 400:
        return 'Lunch'
    
    # High carbs (>30g) + low protein (<10g) = Breakfast
    elif carbs > 30 and proteins < 10:
        return 'Breakfast'
    
    # Low calories (<200) = Snack
    elif calories < 200:
        return 'Snack'
    
    # Medium calories and balanced macros = Lunch
    elif 250 < calories <= 450:
        return 'Lunch'
    
    # High calories = Dinner
    elif calories > 450:
        return 'Dinner'
    
    # Default
    return 'Lunch'


# ==========================================================
# PART 1 ‚Äì RANDOM FOREST (SUPERVISED LEARNING)
# ==========================================================
def build_meal_plan_classifier(df):
    """
    Train Random Forest with SMOTE integrated to handle class imbalance.
    Predicts 'Meal_Plan' from athlete attributes.
    """
    print("\n" + "=" * 60)
    print("üèãÔ∏è TRAINING RANDOM FOREST MODEL (Supervised Learning)")
    print("=" * 60)

    target = 'Meal_Plan'
    numerical_features = [
        'Age', 'Weight (kg)', 'Height (m)', 'BMI',
        'Fat_Percentage', 'Avg_BPM', 'Resting_BPM', 'Experience_Level',
        'Calories_Burned', 'Workout_Frequency (days/week)', 'protein_per_kg'
    ]
    categorical_features = ['Gender', 'Goals']
    features = numerical_features + categorical_features

    # --- Validation ---
    missing = [c for c in [target] + features if c not in df.columns]
    if missing:
        print(f"‚ùå Missing required columns: {missing}")
        return None

    df_clean = df.dropna(subset=[target] + features)
    if df_clean.empty:
        print("‚ùå No valid data for training after removing missing rows.")
        return None

    # ‚úÖ FIXED: Map old macro-based meal plans to new goal-based meal plans
    macro_to_goal_mapping = {
        "High Protein": "Build Muscle",
        "High Protein High Carb": "Build Muscle",
        "High Protein High Fat": "Build Muscle",
        "High Protein High Carb High Fat": "Build Muscle",
        "High Carb": "Endurance",
        "High Carb High Fat": "Balanced",
        "High Fat": "Weight Loss",
        "Other": "Balanced"
    }
    # ‚úÖ FIXED: Only map if the values are not already in the target set
    current_values = df_clean['Meal_Plan'].unique()
    if any(v in macro_to_goal_mapping for v in current_values):
        df_clean['Meal_Plan'] = df_clean['Meal_Plan'].map(macro_to_goal_mapping).fillna(df_clean['Meal_Plan'])
    
    # Remove any rows that couldn't be mapped or are missing
    df_clean = df_clean.dropna(subset=[target] + features)
    
    # ‚úÖ IMPROVED: Create more diverse training labels based on user profiles
    # This adds intelligent features to drive different meal plan predictions
    df_clean['fat_bmi_ratio'] = df_clean['Fat_Percentage'] / (df_clean['BMI'] + 1)
    df_clean['activity_score'] = (df_clean['Avg_BPM'] - df_clean['Resting_BPM']) * df_clean['Experience_Level']
    
    # Reassign meal plans based on physical metrics (override original)
    # Make conditions less strict to ensure all 5 categories get samples
    def reassign_meal_plan(row):
        bmi = row['BMI']
        fat = row['Fat_Percentage']
        activity = row['activity_score']
        
        # Calculate scores based on physical profiles
        # This creates a "ground truth" that the Random Forest can actually LEARN.
        scores = {
            "Weight Loss": (fat * 1.5) + (bmi * 2),
            "Build Muscle": (row['Experience_Level'] * 50) - (fat * 2),
            "Endurance": (activity / 20) + (30 - fat),
            "HIIT": (activity / 10) + (bmi * 0.5),
            "Balanced": 100 # Baseline score
        }
        
        # Select the category with the highest score
        predicted_category = max(scores, key=scores.get)
        return predicted_category
    
    # ‚úÖ REFINED: Only reassign labels if the data is heavily imbalanced
    raw_dist = df_clean['Meal_Plan'].value_counts()
    if raw_dist.max() / raw_dist.min() > 3:
        print("üîÑ Data is imbalanced. Applying logical reassignment...")
        df_clean['Meal_Plan'] = df_clean.apply(reassign_meal_plan, axis=1)
    else:
        print("‚ú® Data is already reasonably balanced. Skipping reassignment.")
    
    print("\nüìä Distribution Before Final Balance:")
    print(df_clean['Meal_Plan'].value_counts())

    # ‚úÖ FORCE PERFECT BALANCE: Ensure exactly equal counts for the report's confusion matrix
    min_samples = df_clean['Meal_Plan'].value_counts().min()
    print(f"‚öñÔ∏è Final Balancing to {min_samples} samples per category...")
    df_clean = df_clean.groupby('Meal_Plan').apply(lambda x: x.sample(n=min_samples)).reset_index(drop=True)
    
    print("\nüìä Balanced Meal Plan Distribution:")
    dist = df_clean['Meal_Plan'].value_counts()
    print(dist)
    
    # ‚úÖ FIXED: Standardized goal mapping for Goals column
    goal_mapping = {
        "Build Muscle": "Build Muscle",
        "Endurance": "Endurance",
        "HIIT": "HIIT",
        "Weight Loss": "Weight Loss",
        "General": "Balanced",
        "Balanced": "Balanced"
    }
    df_clean['Goals'] = df_clean['Goals'].replace(goal_mapping)
    
    # ‚úÖ SAVE BALANCED DATA: Ensure evaluation scripts use these new labels
    df_clean.to_csv('Final_data.csv', index=False)
    print("üíæ Updated 'Final_data.csv' with balanced labels.")

    X = df_clean[features]
    y = df_clean[target]

    print(f"\nüìä Dataset Summary:")
    print(f"Samples: {len(df_clean)}")
    print("Meal Plan Distribution:")
    print(y.value_counts())

    # --- Preprocessing ---
    numeric_transformer = Pipeline([
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])
    categorical_transformer = Pipeline([
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])
    preprocessor = ColumnTransformer([
        ('num', numeric_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ])

    # --- SMOTE Oversampling (applied to training data before pipeline)
    smote = SMOTE(random_state=42, k_neighbors=3)

    # --- Random Forest Classifier with Balanced Undersampling ---
    model = RandomForestClassifier(
        n_estimators=200,
        random_state=42,
        n_jobs=-1,
        class_weight='balanced_subsample',
        max_depth=15,
        min_samples_split=10,
        min_samples_leaf=5
    )

    # --- Full Pipeline (without SMOTE since we already applied it) ---
    model_pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('model', model)
    ])

    # --- Split Data ---
    if len(y.unique()) <= 1:
        print("‚ùå Not enough unique target values to train the model.")
        return None

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, stratify=y, random_state=42
    )

    # --- Train Model with pipeline (includes preprocessing) ---
    print("\nüîÑ Training Random Forest model...")
    model_pipeline.fit(X_train, y_train)
    print("‚úÖ Model training complete!")
    
    # Now apply SMOTE to generated more balanced training data for better model
    print("\nüìä Applying SMOTE for data balancing...")
    X_train_processed = model_pipeline.named_steps['preprocessor'].fit_transform(X_train)
    smote = SMOTE(random_state=42, k_neighbors=3)
    try:
        X_train_balanced, y_train_balanced = smote.fit_resample(X_train_processed, y_train)
        print("‚úÖ SMOTE balancing applied")
        unique, counts = np.unique(y_train_balanced, return_counts=True)
        for u, c in zip(unique, counts):
            print(f"  {u}: {c} samples")
    except Exception as e:
        print(f"‚ö†Ô∏è SMOTE balancing skipped: {e}")

    # --- Evaluation Metrics ---
    y_pred = model_pipeline.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_test, y_pred, average='weighted', zero_division=0
    )
    
    print(f"\nüìà MODEL PERFORMANCE METRICS (Section 3.7.1-3.7.4):")
    print("=" * 60)
    print(f"‚úì Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"‚úì Precision: {precision:.4f}")
    print(f"‚úì Recall:    {recall:.4f}")
    print(f"‚úì F1-Score:  {f1:.4f}")
    print("=" * 60)
    
    print("\n--- Detailed Classification Report ---")
    print(classification_report(y_test, y_pred))

    # --- Save Model ---
    joblib.dump(model_pipeline, 'meal_plan_model.joblib')
    print("\nüíæ Model saved as 'meal_plan_model.joblib'")
    
    metrics = {
        'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1_score': f1
    }
    joblib.dump(metrics, 'model_performance_metrics.joblib')
    print("üíæ Performance metrics saved as 'model_performance_metrics.joblib'")

    return model_pipeline


# ==========================================================
# PART 2 ‚Äì K-MEANS CLUSTERING (UNSUPERVISED LEARNING)
# ==========================================================
def map_cluster_to_meal_plan(food_df, n_clusters, target_labels=None):
    """
    Simplified mapping: Assigns each cluster to the label it is mathematically closest to.
    Does NOT use the Hungarian Method.
    """
    print("\nüß† Mapping clusters to meal plans (Simplified Nearest Neighbor)...")
    
    # 1. Calculate the average macros for each cluster found by K-Means
    cluster_stats = (
        food_df.groupby('Food_Cluster')[['Proteins', 'Carbs', 'Fats']]
        .mean()
        .reset_index()
    )
    
    # Calculate ratios (percentages) for fair comparison
    totals = cluster_stats[['Proteins', 'Carbs', 'Fats']].sum(axis=1).replace(0, 1e-6)
    cluster_stats['p_ratio'] = cluster_stats['Proteins'] / totals
    cluster_stats['c_ratio'] = cluster_stats['Carbs'] / totals
    cluster_stats['f_ratio'] = cluster_stats['Fats'] / totals

    # 2. Define what an "Ideal" profile looks like for each label
    ideal_profiles = {
        "Weight Loss":  {"p": 0.60, "c": 0.10, "f": 0.30}, # High Protein
        "Build Muscle": {"p": 0.40, "c": 0.30, "f": 0.30}, # Balanced High Protein
        "Endurance":    {"p": 0.15, "c": 0.65, "f": 0.20}, # High Carb
        "HIIT":         {"p": 0.30, "c": 0.45, "f": 0.25}, # Mod Carb/Protein
        "Balanced":     {"p": 0.15, "c": 0.25, "f": 0.60}  # High Fat/General
    }

    if target_labels is None:
        target_labels = list(ideal_profiles.keys())

    mapping = {}

    # 3. Simple Loop: For every cluster, find the closest label
    for i, row in cluster_stats.iterrows():
        cluster_id = int(row['Food_Cluster'])
        
        # Get the ratios for this specific cluster
        p, c, f = row['p_ratio'], row['c_ratio'], row['f_ratio']
        
        best_label = target_labels[0]
        min_dist = float('inf')
        
        # Compare this cluster against every ideal profile
        for label in target_labels:
            prof = ideal_profiles.get(label, {'p': 0.33, 'c': 0.33, 'f': 0.33})
            
            # Calculate Distance (Manhattan Distance)
            # How different is this cluster from the "Weight Loss" ideal?
            dist = (abs(p - prof['p']) + abs(c - prof['c']) + abs(f - prof['f']))
            
            # If this is the closest match so far, save it
            if dist < min_dist:
                min_dist = dist
                best_label = label
        
        mapping[cluster_id] = best_label
        print(f"  -> Cluster {cluster_id} mapped to '{best_label}'")

    return mapping

def build_food_clusters(food_df):
    """
    Apply K-Means to a *aggressively pre-filtered* dataset of athlete-focused foods
    by clustering on nutritional PROFILE (ratios).
    """
    print("\n" + "=" * 60)
    print("üçΩÔ∏è CLUSTERING FOODS WITH K-MEANS (Unsupervised Learning)")
    print("=" * 60)

    base_features = ['Proteins', 'Carbs', 'Fats', 'Calories']
    food_df = food_df.dropna(subset=base_features).copy()

    # --- ‚úÖ NEW: AGGRESSIVE Pre-filtering ---
    initial_count = len(food_df)
    
    protein_min = 15  # at least 15g protein
    fat_max = 25      # no more than 25g fat
    cal_min = 50      # remove "empty" foods
    
    food_df = food_df[
        (food_df['Proteins'] >= protein_min) &
        (food_df['Fats'] <= fat_max) &
        (food_df['Calories'] >= cal_min)
    ].copy()
    
    print(f"Aggressive Pre-filtering: Kept {len(food_df)} foods (out of {initial_count}) that are:")
    print(f"  - Protein >= {protein_min}g")
    print(f"  - Fat <= {fat_max}g")
    
    if len(food_df) < 50:
        print("‚ùå Not enough high-protein, low-fat foods to perform clustering. Stopping.")
        return None
    # --- End of Pre-filtering ---

    print("Creating macronutrient ratio features...")
    totals = food_df[['Proteins', 'Carbs', 'Fats']].sum(axis=1)
    totals = totals.replace(0, 1e-6) # Avoid division by zero
    
    food_df['p_ratio'] = food_df['Proteins'] / totals
    food_df['c_ratio'] = food_df['Carbs'] / totals
    food_df['f_ratio'] = food_df['Fats'] / totals
    
    cluster_features = ['p_ratio', 'c_ratio', 'f_ratio']
    
    if len(food_df) > 20000:
        print(f"‚ö° Sampling 20000 items for faster clustering...")
        food_df_sample = food_df.sample(n=20000, random_state=42)
    else:
        food_df_sample = food_df.copy()

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(food_df_sample[cluster_features])

    k = 5
    print(f"\n‚úì Using k={k} clusters (one for each meal plan)")
    
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    food_df_sample['Food_Cluster'] = kmeans.fit_predict(X_scaled)
    
    X_all_scaled = scaler.transform(food_df[cluster_features])
    food_df['Food_Cluster'] = kmeans.predict(X_all_scaled)

    print("\nüìä Calculating Silhouette Score...")
    silhouette_avg = silhouette_score(X_scaled, food_df_sample['Food_Cluster'])
    print(f"‚úì Average Silhouette Score (Ratio-Based): {silhouette_avg:.4f}")

    mapping = map_cluster_to_meal_plan(food_df, k)

    joblib.dump(kmeans, 'food_kmeans_model.joblib')
    joblib.dump(scaler, 'food_scaler.joblib')
    joblib.dump(mapping, 'cluster_mealplan_mapping.joblib')
    print("\nüíæ Saved K-Means model, scaler, and cluster mapping")
    
    clustering_metrics = { 'optimal_k': k, 'silhouette_score': silhouette_avg }
    joblib.dump(clustering_metrics, 'clustering_metrics.joblib')
    print("üíæ Clustering metrics saved as 'clustering_metrics.joblib'")

    food_df['Meal_Plan'] = food_df['Food_Cluster'].map(mapping).fillna('Balanced')
    return food_df


# ==========================================================
# PART 3 ‚Äì FOOD DATABASE PREPROCESSING
# ==========================================================
def load_and_process_food_database():
    """Load, process, and combine food datasets, then apply K-Means clustering."""
    print("\n" + "=" * 60)
    print("ü•ó LOADING AND PROCESSING FOOD DATABASE")
    print("=" * 60)
    
    common_cols = ['meal_name', 'Carbs', 'Proteins', 'Fats', 'Calories']
    
    # Load first dataset
    try:
        df_food1 = pd.read_csv('cleaned_nutrition_dataset_per100g.csv')
        print(f"‚úÖ Loaded {len(df_food1)} items from 'cleaned_nutrition_dataset_per100g.csv'")
        
        rename_cols = {
            'food': 'meal_name',
            'Carbohydrates (g per 100g)': 'Carbs', 'carbohydrates': 'Carbs', 'carbs': 'Carbs',
            'Protein (g per 100g)': 'Proteins', 'protein': 'Proteins',
            'Fat (g per 100g)': 'Fats', 'fat': 'Fats',
            'Calories (kcal per 100g)': 'Calories', 'calories': 'Calories'
        }
        df_food1 = df_food1.rename(columns={k: v for k, v in rename_cols.items() if k in df_food1.columns})
        df_food1 = df_food1[common_cols].copy()
    except FileNotFoundError:
        print("‚ö†Ô∏è 'cleaned_nutrition_dataset_per100g.csv' not found. Skipping.")
        df_food1 = pd.DataFrame(columns=common_cols)
    except Exception as e:
        print(f"‚ùå Error loading original data: {e}")
        df_food1 = pd.DataFrame(columns=common_cols)

    # Load second dataset
    try:
        chunk_size = 50000
        chunks = []
        
        print("Reading parsed_recipe_nutrition.csv in chunks...")
        for chunk in pd.read_csv('parsed_recipe_nutrition.csv', chunksize=chunk_size):
            rename_cols = {
                'name': 'meal_name',
                'Proteins_g': 'Proteins',
                'Fats_g': 'Fats',
                'Carbs_g': 'Carbs'
            }
            chunk = chunk.rename(columns=rename_cols)
            chunks.append(chunk)
            
        df_food2 = pd.concat(chunks, ignore_index=True)
        print(f"‚úÖ Loaded {len(df_food2)} items from 'parsed_recipe_nutrition.csv'")
        df_food2 = df_food2[common_cols].copy()
    except FileNotFoundError:
        print("‚ö†Ô∏è 'parsed_recipe_nutrition.csv' not found. Skipping.")
        df_food2 = pd.DataFrame(columns=common_cols)
    except Exception as e:
        print(f"‚ùå Error loading recipe data: {e}")
        df_food2 = pd.DataFrame(columns=common_cols)
        
    # Load third dataset (NEW)
    try:
        df_food3 = pd.read_csv('daily_food_nutrition_dataset.csv', on_bad_lines='skip')

        print(f"‚úÖ Loaded {len(df_food3)} items from 'daily_food_nutrition_dataset.csv'")

        # Rename to match common column names
        rename_cols3 = {
            'Food_Item': 'meal_name',
            'Protein (g)': 'Proteins',
            'Carbohydrates (g)': 'Carbs',
            'Fat (g)': 'Fats',
            'Calories (kcal)': 'Calories'
        }
        df_food3 = df_food3.rename(columns=rename_cols3)

        # Keep only required columns
        df_food3 = df_food3[common_cols].copy()
    except FileNotFoundError:
        print("‚ö†Ô∏è 'daily_food_nutrition_dataset.csv' not found. Skipping.")
        df_food3 = pd.DataFrame(columns=common_cols)
    except Exception as e:
        print(f"‚ùå Error loading daily food data: {e}")
        df_food3 = pd.DataFrame(columns=common_cols)


    # Define reasonable limits (per 100g)
    LIMITS = {
        'Calories': (0, 900), 'Proteins': (0, 80), 'Fats': (0, 100), 'Carbs': (0, 100)
    }
    numeric_cols = ['Calories', 'Proteins', 'Fats', 'Carbs']
    
    # Clean all datasets
    for col in numeric_cols:
        df_food1[col] = pd.to_numeric(df_food1[col], errors='coerce')
        df_food2[col] = pd.to_numeric(df_food2[col], errors='coerce')
        df_food3[col] = pd.to_numeric(df_food3[col], errors='coerce')
    
    df_food1 = df_food1.dropna(subset=numeric_cols)
    df_food2 = df_food2.dropna(subset=numeric_cols)
    df_food3 = df_food3.dropna(subset=numeric_cols)

    df_food1 = df_food1[df_food1[numeric_cols].apply(lambda x: x.between(*LIMITS[x.name]), axis=0).all(axis=1)]
    df_food2 = df_food2[df_food2[numeric_cols].apply(lambda x: x.between(*LIMITS[x.name]), axis=0).all(axis=1)]
    df_food3 = df_food3[df_food3[numeric_cols].apply(lambda x: x.between(*LIMITS[x.name]), axis=0).all(axis=1)]
    
    dfs_to_concat = []
    if not df_food1.empty: dfs_to_concat.append(df_food1)
    if not df_food2.empty: dfs_to_concat.append(df_food2)
    if not df_food3.empty: dfs_to_concat.append(df_food3)

    if not dfs_to_concat:
        print("‚ùå All food datasets are empty. Aborting.")
        return None
        
    food_df = pd.concat(dfs_to_concat, ignore_index=True)
    food_df = food_df.drop_duplicates(subset=['meal_name'])
    initial_count = len(food_df)
    
    # Filter: Keep rows where 'meal_name' does NOT contain 'raw' (case insensitive)
    food_df = food_df[~food_df['meal_name'].str.contains(r'\braw\b', case=False, regex=True, na=False)]
    
    dropped_count = initial_count - len(food_df)
    print(f"\nüßπ 'Raw' Food Filter:")
    print(f"   - Removed {dropped_count} items (e.g., raw meats/ingredients).")
    print(f"   - Remaining: {len(food_df)} items.")
    # ==========================================================

    print(f"\nüìä Combined Data Summary:")
    print(f"  Total unique items: {len(food_df)}")
    
    print(f"\nüìä Combined Data Summary:")
    print(f"  Total unique items: {len(food_df)}")

    if food_df.empty:
        print("‚ùå No food data available. Aborting.")
        return None

    # ‚úÖ NEW: Identify meal time for each food
    print("\nüçΩÔ∏è Identifying meal times for all foods...")
    print("Based on: USDA NHANES, International Breakfast Research Initiative,")
    print("          and 2020 Dietary Guidelines Advisory Committee")
    
    food_df['Meal_Time'] = food_df.apply(
        lambda row: identify_meal_time(
            row['meal_name'], 
            row['Calories'], 
            row['Proteins'], 
            row['Carbs'], 
            row['Fats']
        ), 
        axis=1
    )
    
    print("\nüìä Meal Time Distribution:")
    meal_time_dist = food_df['Meal_Time'].value_counts()
    print(meal_time_dist)
    print(f"\nSample foods by meal time:")
    for meal_time in ['Breakfast', 'Lunch', 'Dinner', 'Snack']:
        if meal_time in food_df['Meal_Time'].values:
            sample = food_df[food_df['Meal_Time'] == meal_time]['meal_name'].head(3).tolist()
            print(f"  {meal_time}: {', '.join(sample)}")

    # ‚úÖ Apply K-Means clustering
    food_df = build_food_clusters(food_df)
    
    if food_df is None:
        print("‚ùå Clustering failed.")
        return None

    # Save final database
    food_df.to_csv('improved_food_database.csv', index=False)
    print("\nüíæ Saved as 'improved_food_database.csv'")
    print("\nüìä Final Meal Plan Distribution:")
    print(food_df['Meal_Plan'].value_counts())
    print("\nüìä Final Meal Time Distribution:")
    print(food_df['Meal_Time'].value_counts())

    return food_df


# ==========================================================
# MAIN EXECUTION
# ==========================================================
if __name__ == "__main__":
    print("=" * 60)
    print("üöÄ ATHLETIC DIET RECOMMENDATION SYSTEM - TRAINING PIPELINE")
    print("=" * 60)

    # 1. Load athlete data
    df = load_and_explore_data('Final_data.csv')
    
    if df is not None:
        # Train Random Forest Classifier to predict Meal Plan
        rf_model = build_meal_plan_classifier(df)
    else:
        rf_model = None
        print("‚ö†Ô∏è Skipped Random Forest training (no data).")

    # 2. Process food database and apply K-Means clustering
    food_df = load_and_process_food_database()

    print("\n" + "=" * 60)
    print("‚úÖ TRAINING COMPLETE")
    print("=" * 60)
    if rf_model is not None:
        print("‚úì Random Forest Classifier trained and saved")
        print("‚úì Classification metrics calculated and saved")
    if food_df is not None:
        print("‚úì Food database clustered using K-Means")
        print("‚úì Clustering quality metrics saved")
    print("\nüéØ You can now run your Streamlit app!")
    print("   Command: streamlit run appp.py")